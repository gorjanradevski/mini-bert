{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-curtis",
   "metadata": {},
   "source": [
    "# Mini-BERT: Inspired by and adapted from [MinGPT](https://github.com/karpathy/minGPT/) and [HuggingFace BERT](https://huggingface.co/transformers/model_doc/bert.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-balance",
   "metadata": {},
   "source": [
    "## One config object to initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # One config object to initalize everything\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30000,\n",
    "        max_seq_len=256,\n",
    "        hidden_size=768,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        drop_prob=0.1,\n",
    "        token_types=2,\n",
    "    ):\n",
    "        # Number of words in the vocabulary\n",
    "        self.vocab_size = vocab_size\n",
    "        # The maximal number of tokens the model can accept as input\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # The hidden size of the model\n",
    "        self.hidden_size = hidden_size\n",
    "        # The number of layers in the model\n",
    "        self.num_layers = num_layers\n",
    "        # The number of self-attention heads\n",
    "        self.num_heads = num_heads\n",
    "        # Dropout probs\n",
    "        self.drop_prob = drop_prob\n",
    "        # Types of tokens accepted by the transformer\n",
    "        self.token_types = token_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-differential",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention and multi-head attention\n",
    "\n",
    "![Multi-head attention](imgs/attention.png)\n",
    "\n",
    "Image source: [Vaswani et al., 2017. Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        # Layers for projecting the queries, keys and values\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        # Dropout layers\n",
    "        self.attn_drop = nn.Dropout(config.drop_prob)\n",
    "        self.resid_drop = nn.Dropout(config.drop_prob)\n",
    "        # Projection layer\n",
    "        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.num_heads = config.num_heads\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        # (Batch size, Seq. len., Num. heads, Hidden size)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, hidden_size // self.num_heads)\n",
    "        # (Batch size, Num. heads, Seq. len., Hidden size)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        # Obtain dimensions\n",
    "        batch_size, seq_len, hidden_size = inputs.size()\n",
    "        # Project queries, keys and values\n",
    "        keys = self.key(inputs)\n",
    "        queries = self.query(inputs)\n",
    "        values = self.value(inputs)\n",
    "        # Reshape and transpose to prepare for dot-product attention\n",
    "        keys = self.transpose_for_scores(keys)\n",
    "        queries = self.transpose_for_scores(queries)\n",
    "        values = self.transpose_for_scores(values)\n",
    "        # Self-attention\n",
    "        # (BS, NH, SL., HS) x (BS, NH, HS, SL) -> (BS, NH, SL, SL)\n",
    "        attn = queries @ keys.transpose(-2, -1)\n",
    "        # Scale\n",
    "        attn = attn * (1.0 / math.sqrt(keys.size(-1)))\n",
    "        # Set the scores of the padding tokens to -infinity\n",
    "        attn.masked_fill_(padding_mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        # Normalize accross the last dimension\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper\n",
    "        attn = self.attn_drop(attn)\n",
    "        # Attend\n",
    "        # (BS, NH, SL, SL) x (BS, NH, SL, HS) -> (BS, NH, SL, HS)\n",
    "        output = attn @ values\n",
    "        # (Batch size, Seq. len., Num. heads, Hidden size)\n",
    "        output = output.transpose(1, 2)\n",
    "        # Concatenate all the heads one next to each other\n",
    "        # (Batch size, Seq. len., Hidden size)\n",
    "        output = output.contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        # Project\n",
    "        output = self.proj(output)\n",
    "        output = self.resid_drop(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-explosion",
   "metadata": {},
   "source": [
    "![Transformer encoder](imgs/encoder.png)\n",
    "\n",
    "Image source: [Vaswani et al., 2017. Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = MultiHeadSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.hidden_size, config.hidden_size),\n",
    "            nn.Dropout(config.drop_prob),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        # Multi-head self-attention + Add & Norm\n",
    "        inputs = self.ln1(inputs + self.attn(inputs, padding_mask))\n",
    "        # Feed-forward + Add & norm\n",
    "        inputs = self.ln2(inputs + self.mlp(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-easter",
   "metadata": {},
   "source": [
    "## BERT embeddings\n",
    "\n",
    "![Bert embeddings](imgs/bert_embeddings.png)\n",
    "\n",
    "Image source: [Devlin et al., 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    # Adapted from:\n",
    "    # https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L165\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Yellow\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        # Grey\n",
    "        self.position_embeddings = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        # Green\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.token_types, config.hidden_size\n",
    "        )\n",
    "        # Not included: LayerNorm and Dropout\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.drop_prob)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        position_ids: torch.Tensor,\n",
    "        token_type_ids: torch.Tensor,\n",
    "    ):\n",
    "        # (Batch size, Seq. len., Hidden size)\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        pos_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        # Summing all three together\n",
    "        embeddings = inputs_embeds + pos_embeds + token_type_embeds\n",
    "        # Normalize and dropout\n",
    "        embeddings = self.ln(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-stevens",
   "metadata": {},
   "source": [
    "## Defining the complete model: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # BERT embeddings\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        # Transformer encoder\n",
    "        self.encoder = nn.ModuleList([Layer(config) for _ in range(config.num_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        position_ids: torch.Tensor,\n",
    "        token_type_ids: torch.Tensor,\n",
    "        padding_mask: torch.Tensor,\n",
    "    ):\n",
    "        # Obtain the embeddings\n",
    "        hidden_states = self.embeddings(input_ids, position_ids, token_type_ids)\n",
    "        # Pass the embeddings through the Transformer encoder\n",
    "        for layer in self.encoder:\n",
    "            hidden_states = layer(hidden_states, padding_mask)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-throat",
   "metadata": {},
   "source": [
    "## Testing everything with dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "bert = BERT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = torch.tensor([[1, 2, 3, 4], [8, 8, 10, 15]], dtype=torch.long)\n",
    "    position_ids = torch.arange(4, dtype=torch.long).unsqueeze(0).repeat(2, 1)\n",
    "    token_ids = torch.ones(2, 4, dtype=torch.long)\n",
    "    padding_mask = torch.tensor(\n",
    "        [[False, False, False, True], [False, False, True, True]]\n",
    "    )\n",
    "    outputs = bert(input_ids, position_ids, token_ids, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-translation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
